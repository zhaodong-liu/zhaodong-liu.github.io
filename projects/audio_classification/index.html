<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Music Snippet Classification | Zhaodong Liu </title> <meta name="author" content="Zhaodong Liu"> <meta name="description" content="Ensemble of ResNet34 models for classifying music audio snippets by singer gender"> <meta name="keywords" content="machine-learning, natural-language-processing, recommendation-systems, data-science, computer-science, mathematics"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://zhaodong-liu.github.io/projects/audio_classification/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Zhaodong</span> Liu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Music Snippet Classification</h1> <p class="post-description">Ensemble of ResNet34 models for classifying music audio snippets by singer gender</p> </header> <article> <h2 id="overview">Overview</h2> <p>This project implements an ensemble of deep learning models to classify 3-second audio snippets into 4 gender-based categories, achieving 80.42% accuracy. The pipeline combines vocal separation, custom mel-frequency feature extraction, and ensemble learning techniques.</p> <p><strong>Course:</strong> CSCI-SHU 360 Machine Learning</p> <p><strong>Institution:</strong> New York University Shanghai</p> <p><strong>Final Accuracy:</strong> 80.42%</p> <p><strong>Repository:</strong> <a href="https://github.com/zhaodong-liu/Audio-Classification" rel="external nofollow noopener" target="_blank">GitHub - Audio-Classification</a></p> <p><strong>Report:</strong> <a href="/assets/pdf/audio_classification_report.pdf">Final Report (PDF)</a></p> <h2 id="problem-statement">Problem Statement</h2> <p>The challenge was to classify short music snippets (3 seconds) into 4 categories based on singer gender. This task required distinguishing vocal characteristics while handling:</p> <ul> <li>Background instrumentation that obscures vocal features</li> <li>Limited temporal context (only 3 seconds per sample)</li> <li>High inter-class similarity in vocal characteristics</li> <li>Need for robust feature representation from raw audio</li> </ul> <h2 id="methodology">Methodology</h2> <h3 id="1-preprocessing-pipeline">1. Preprocessing Pipeline</h3> <h4 id="vocal-separation">Vocal Separation</h4> <ul> <li>Utilized <strong>Spleeter</strong> to isolate vocal tracks from background music</li> <li>Achieved approximately 5% accuracy improvement</li> <li>Focused model attention on singer characteristics rather than instrumentation</li> </ul> <h4 id="feature-extraction">Feature Extraction</h4> <p>Implemented custom mel-frequency filter bank feature extraction:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Configuration
</span><span class="n">Sample</span> <span class="n">rate</span><span class="p">:</span> <span class="mi">16</span><span class="n">kHz</span>
<span class="n">Segment</span> <span class="n">length</span><span class="p">:</span> <span class="mi">3</span> <span class="n">seconds</span>
<span class="n">Pre</span><span class="o">-</span><span class="n">emphasis</span> <span class="nb">filter</span><span class="p">:</span> <span class="n">α</span> <span class="o">=</span> <span class="mf">0.97</span>
<span class="n">Frame</span> <span class="n">size</span><span class="p">:</span> <span class="mi">25</span><span class="nf">ms </span><span class="p">(</span><span class="mi">400</span> <span class="n">samples</span><span class="p">)</span>
<span class="n">Frame</span> <span class="n">stride</span><span class="p">:</span> <span class="mi">10</span><span class="nf">ms </span><span class="p">(</span><span class="mi">160</span> <span class="n">samples</span><span class="p">)</span>
<span class="n">Window</span><span class="p">:</span> <span class="n">Hamming</span>
<span class="n">FFT</span> <span class="n">size</span><span class="p">:</span> <span class="mi">1024</span>
<span class="n">Mel</span> <span class="nb">filter</span> <span class="n">banks</span><span class="p">:</span> <span class="mi">40</span> <span class="nf">banks </span><span class="p">(</span><span class="mi">0</span><span class="o">-</span><span class="mi">8000</span> <span class="n">Hz</span><span class="p">)</span>
<span class="n">Output</span> <span class="n">shape</span><span class="p">:</span> <span class="p">(</span><span class="mi">299</span><span class="p">,</span> <span class="mi">40</span><span class="p">)</span> <span class="n">per</span> <span class="n">sample</span>
</code></pre></div></div> <p>Rather than using pre-built MFCC functions, I implemented the mel-frequency filter bank computation from scratch, providing:</p> <ul> <li>Fine-grained control over audio representation</li> <li>Custom optimization for 3-second snippets</li> <li>Better understanding of the feature extraction process</li> </ul> <p>Features are saved as individual <code class="language-plaintext highlighter-rouge">.npz</code> files for efficient loading during training.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/audio_classification/weight_computation-480.webp 480w,/assets/img/projects/audio_classification/weight_computation-800.webp 800w,/assets/img/projects/audio_classification/weight_computation-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/audio_classification/weight_computation.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Weight Computation" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Mel filter bank weight computation and application process </div> <h3 id="2-model-architecture">2. Model Architecture</h3> <p><strong>Base Model: ResNet34</strong></p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/audio_classification/resnet34-480.webp 480w,/assets/img/projects/audio_classification/resnet34-800.webp 800w,/assets/img/projects/audio_classification/resnet34-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/audio_classification/resnet34.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="ResNet34 Architecture" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> ResNet34 architecture adapted for audio classification </div> <p>Architecture modifications:</p> <ul> <li>Modified input layer to accept 1-channel spectrograms (299×40)</li> <li>Added dropout layer (p=0.2) before final classification</li> <li>4-class output layer</li> <li>Trained from scratch (no pre-trained weights)</li> </ul> <p><strong>Ensemble Method</strong></p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/audio_classification/ensemble1-480.webp 480w,/assets/img/projects/audio_classification/ensemble1-800.webp 800w,/assets/img/projects/audio_classification/ensemble1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/audio_classification/ensemble1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Ensemble Architecture" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Ensemble of 20 independent ResNet34 models with averaged predictions </div> <ul> <li>20 independent ResNet34 models</li> <li>Each model trained on the same data with different shuffling</li> <li>Predictions averaged across all models</li> <li>Ensemble improved accuracy by 2-3% over single models</li> </ul> <h3 id="3-training-configuration">3. Training Configuration</h3> <p><strong>Optimizer &amp; Loss:</strong></p> <ul> <li>Optimizer: AdamW (lr=3e-4, weight_decay=1e-5)</li> <li>Loss Function: CrossEntropyLoss</li> <li>Learning Rate Scheduler: CosineAnnealingWarmRestarts</li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/audio_classification/lr_scheduler-480.webp 480w,/assets/img/projects/audio_classification/lr_scheduler-800.webp 800w,/assets/img/projects/audio_classification/lr_scheduler-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/audio_classification/lr_scheduler.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Learning Rate Schedule" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Cosine annealing with warm restarts learning rate schedule </div> <p><strong>Training Details:</strong></p> <ul> <li>Batch Size: 32</li> <li>Epochs: 10-20</li> <li>Train/Validation Split: 70/30 (random_state=8)</li> <li>Platform: Kaggle (GPU acceleration)</li> </ul> <p><strong>Advanced Techniques:</strong></p> <ul> <li>Dropout (rate=0.2) to prevent overfitting</li> <li>Cosine annealing with warm restarts for learning rate scheduling</li> <li>20-model ensemble for robust predictions</li> <li>Explored mixup augmentation (experimental)</li> </ul> <h2 id="dataset">Dataset</h2> <ul> <li> <strong>Training Set:</strong> 11,886 audio samples (3 seconds each)</li> <li> <strong>Test Set:</strong> 2,447 audio samples</li> <li> <strong>Categories:</strong> 4 classes (gender-based classification)</li> <li> <strong>Format:</strong> MP3 → Vocal separation → Mel filter banks → NPZ</li> </ul> <h2 id="results">Results</h2> <table> <thead> <tr> <th>Model</th> <th>Accuracy</th> </tr> </thead> <tbody> <tr> <td>Single CNN (3 conv layers)</td> <td>~65%</td> </tr> <tr> <td>Single ResNet34</td> <td>~74%</td> </tr> <tr> <td>ResNet34 Ensemble (4-5 models)</td> <td>~77-78%</td> </tr> <tr> <td><strong>ResNet34 Ensemble (20 models)</strong></td> <td><strong>80.42%</strong></td> </tr> </tbody> </table> <p>The ensemble approach with 20 models provided the best performance. Key improvements came from:</p> <ol> <li>Vocal separation preprocessing: ~5% improvement</li> <li>Custom mel-frequency features: Better representation than standard MFCCs</li> <li>Ensemble learning: 2-3% improvement over single models</li> <li>Proper regularization and learning rate scheduling</li> </ol> <h2 id="technical-highlights">Technical Highlights</h2> <ol> <li> <p><strong>Custom Mel Filter Banks:</strong> Hand-implemented mel-frequency feature extraction instead of using pre-built MFCC functions, providing fine-grained control over the audio representation and deeper understanding of the signal processing pipeline.</p> </li> <li> <p><strong>Vocal Isolation:</strong> Spleeter-based vocal separation focuses the model on singer characteristics rather than background instrumentation, significantly improving classification accuracy.</p> </li> <li> <p><strong>Large-Scale Ensemble:</strong> Training 20 independent models and averaging predictions significantly improves robustness and generalization, though at the cost of computational expense.</p> </li> <li> <p><strong>Modified ResNet Architecture:</strong> Successfully adapted computer vision architecture (ResNet34) for audio classification by treating spectrograms as single-channel images.</p> </li> </ol> <h2 id="challenges--lessons-learned">Challenges &amp; Lessons Learned</h2> <p><strong>Challenges:</strong></p> <ul> <li>Limited hyperparameter exploration due to computational constraints</li> <li>Training constrained by Kaggle platform limitations (random disconnections)</li> <li>Balancing ensemble size with computational cost</li> <li>Data augmentation experimentation limited by time constraints</li> </ul> <p><strong>Key Learnings:</strong></p> <ul> <li>Preprocessing (vocal separation) can have major impact on model performance</li> <li>Ensemble methods provide consistent improvements with diminishing returns</li> <li>Custom feature extraction provides better understanding and control</li> <li>Proper learning rate scheduling is crucial for convergence</li> </ul> <h2 id="technical-skills--tools">Technical Skills &amp; Tools</h2> <p><strong>Machine Learning &amp; Deep Learning:</strong></p> <ul> <li>ResNet34 architecture and convolutional neural networks</li> <li>Ensemble learning techniques</li> <li>Transfer learning concepts (adapting vision models for audio)</li> <li>Regularization techniques (dropout, weight decay)</li> <li>Learning rate scheduling (cosine annealing)</li> </ul> <p><strong>Audio Processing:</strong></p> <ul> <li>Mel-frequency filter banks implementation</li> <li>STFT (Short-Time Fourier Transform)</li> <li>Audio feature extraction (MFCCs, spectrograms)</li> <li>Vocal separation using Spleeter</li> </ul> <p><strong>Development &amp; Tools:</strong></p> <ul> <li>PyTorch deep learning framework</li> <li>Librosa for audio processing</li> <li>NumPy for numerical computing</li> <li>Kaggle platform for GPU training</li> <li>Python scientific computing stack</li> </ul> <h2 id="future-directions">Future Directions</h2> <ul> <li> <strong>Hyperparameter Tuning:</strong> Systematic exploration of learning rates, epochs, and model architectures</li> <li> <strong>Advanced Architectures:</strong> Explore transformer-based models or audio-specific architectures like WaveNet</li> <li> <strong>Data Augmentation:</strong> Further experimentation with mixup, SpecAugment, and time-stretching</li> <li> <strong>Model Efficiency:</strong> Investigate knowledge distillation to compress the ensemble into a single model</li> <li> <strong>Real-time Inference:</strong> Optimize for deployment with reduced latency</li> </ul> <h2 id="references">References</h2> <ul> <li>ResNet: <a href="https://arxiv.org/abs/1512.03385" rel="external nofollow noopener" target="_blank">Deep Residual Learning for Image Recognition</a> </li> <li>Spleeter: <a href="https://github.com/deezer/spleeter" rel="external nofollow noopener" target="_blank">Audio Source Separation</a> </li> <li>Mel-Frequency Filter Banks: Standard audio feature extraction technique for speech and music processing</li> </ul> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Zhaodong Liu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>