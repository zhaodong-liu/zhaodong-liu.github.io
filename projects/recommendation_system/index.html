<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Generative Recommendation System | Zhaodong Liu </title> <meta name="author" content="Zhaodong Liu"> <meta name="description" content="A multimodal generative recommendation system that extends ActionPiece with visual features"> <meta name="keywords" content="machine-learning, natural-language-processing, recommendation-systems, data-science, computer-science, mathematics"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://zhaodong-liu.github.io/projects/recommendation_system/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Zhaodong</span> Liu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Generative Recommendation System</h1> <p class="post-description">A multimodal generative recommendation system that extends ActionPiece with visual features</p> </header> <article> <h2 id="overview">Overview</h2> <p>We study how to build more effective recommendation systems using NLP techniques, with a focus on overcoming two long-standing challenges: <strong>long-tail items</strong> and <strong>cold-start users</strong>. Traditional recommendation systems typically follow a candidate-generation-plus-ranking pipeline, which tends to over-favor popular items and ignore long-tail items. They also struggle with cold-start problems, where user interaction histories are short, leading to unreliable candidate filtering and poor personalization in the early stages.</p> <p>To address these limitations, we propose <strong>CLAIRO</strong> (Collaborative Learning with Action-aware Image-text Representation Optimization), a system based on autoregressive sequence modeling. Building on the context-aware tokenization of ActionPiece, we extend multimodal capability for the model, using novel methods to combine visual and textual information. Our goal is to make advanced generative recommendation techniques both practical and scalable for real-world industry applications.</p> <p><strong>Duration:</strong> September 2024 - Present</p> <p><strong>Institution:</strong> New York University Shanghai</p> <p><strong>Advisor:</strong> <a href="https://whongyi.github.io" rel="external nofollow noopener" target="_blank">Prof. Hongyi Wen</a></p> <p><strong>Team Members:</strong></p> <ul> <li>Zhaodong Liu (zl4789@nyu.edu)</li> <li>Yuquan Hu (yh4663@nyu.edu)</li> <li>Tuoye Liu (tl3735@nyu.edu)</li> </ul> <h2 id="research-goals">Research Goals</h2> <p>The primary objectives of <strong>CLAIRO</strong> are to:</p> <ul> <li>Extend ActionPiece’s collaborative tokenization framework with multimodal capabilities</li> <li>Integrate visual and textual information through novel token merging mechanisms</li> <li>Discover co-occurring patterns across text and vision modalities</li> <li>Maintain computational efficiency while improving prediction accuracy</li> </ul> <h2 id="baseline-models">Baseline Models</h2> <p>We conducted a comprehensive literature review and selected three state-of-the-art approaches as our baselines:</p> <h3 id="1-liger">1. LIGER</h3> <p>Implements a hybrid paradigm that unifies dense retrieval and generative retrieval based on the TIGER architecture. By introducing a semantic indexing mechanism and constrained beam search, LIGER achieves superior performance in sequential recommendation tasks. We use LIGER to evaluate RQ-VAE-based quantization approaches in text-only settings.</p> <h3 id="2-mql4grec">2. MQL4GRec</h3> <p>Proposes a multimodal quantitative language framework that extends TIGER-like methods with visual inputs. Their key innovation lies in using Residual-Quantized Variational AutoEncoder (RQ-VAE) to discretize visual features into quantitative tokens, enabling unified processing of text and images through a T5-based encoder-decoder.</p> <h3 id="3-actionpiece-primary-baseline">3. ActionPiece (Primary Baseline)</h3> <p>Introduces a novel collaborative tokenization method that discovers latent behavior patterns through token merging, achieving state-of-the-art results with text-only inputs. Its ability to capture co-occurring user-item interactions through dynamic token clustering makes it an ideal foundation for multimodal extension. We chose ActionPiece as our primary baseline due to its recent superior performance and the potential to enhance it with multimodal capabilities.</p> <h2 id="methodology">Methodology</h2> <h3 id="research-motivation-and-evolution">Research Motivation and Evolution</h3> <p>The core question driving this research is: <strong>how can we effectively learn unified representations of multimodal information (such as human behaviors combined with visual and textual cues) without suffering from codebook collapse or computational inefficiency?</strong></p> <p>Recognizing that human decision-making often relies on visual cues, we developed a recommendation framework using collaborative tokenization to discover latent behavioral patterns across modalities.</p> <h3 id="overcoming-vq-vae-limitations">Overcoming VQ-VAE Limitations</h3> <p>Our initial approach used a <strong>Vector Quantized Variational Autoencoder (VQ-VAE)</strong>, which maps continuous embeddings to nearest codebook vectors. However, this soon collapsed to a small subset of codes, failing to utilize most of the codebook and unable to deal with hierarchical structures—a common property in real-world decision-making processes.</p> <p>After systematic analysis, I recognized this as a structural issue: VQ-VAE’s inherent single-quantized architecture lacked the ability to refine latent information progressively. This led us to explore <strong>residual quantization</strong>, optimizing the <strong>Residual Quantized Variational Autoencoder (RQ-VAE)</strong> which allows each level to refine the remaining semantic residuals, yielding much more interpretable discrete code hierarchies while improving codebook utilization.</p> <h3 id="multimodal-token-merging-architecture">Multimodal Token Merging Architecture</h3> <p>Building upon ActionPiece’s tokenization framework, we successfully extended its capability to incorporate visual features. To learn user behavioral patterns not only from single-item modality, but also from the similarity across textual and visual modalities, we implemented a <strong>byte pair encoding (BPE)-inspired token reconstruction algorithm</strong> that identifies and merges co-occurring multimodal behavioral patterns, effectively aligning latent codes with human-interpretable behavioral units. This improved sequence coherence and codebook utilization.</p> <p>Our implementation integrates:</p> <ol> <li> <strong>Visual Encoder</strong>: CLIP ViT-L/14 extracts image embeddings for product images</li> <li> <strong>Multimodal Fusion with Dimensionality Reduction</strong>: <ul> <li>Visual embeddings from pretrained ViT are compressed via <strong>Principal Component Analysis (PCA)</strong>, preserving <strong>95% of embedding information</strong> while reducing computational overhead by <strong>50%</strong> </li> <li>Reduced visual vectors are concatenated with textual sentence embeddings from SentenceTransformer</li> <li>Optional second-stage PCA ensures the fused representation remains compact and well-conditioned</li> <li> <strong>Learned attention mechanism</strong> effectively integrates visual and textual features, letting the quantizer operate on aligned multimodal semantics rather than raw high-dimensional features</li> </ul> </li> <li> <strong>Joint Token Merging</strong>: Modified ActionPiece’s token merging mechanism to jointly consider both textual and visual tokens during clustering, discovering co-occurring patterns across modalities</li> <li> <strong>FAISS-based Quantization</strong>: Fused embeddings generate product-level semantic codes integrated into ActionPiece’s feature tuple structure</li> </ol> <p>The resulting behavioral tokens encode both textual semantics and visual appearance, providing richer item representations for recommendation tasks.</p> <h3 id="addressing-high-dimensional-challenges">Addressing High-Dimensional Challenges</h3> <p>Integrating multimodal data presented new challenges: high-dimensional embeddings overwhelmed the quantizers, making it difficult to form stable clusters in latent space and leading to overfitting in some dominant areas while significantly increasing computational overhead. The PCA compression and attention mechanism innovations enabled the model to operate reliably while maintaining both representation quality and efficiency.</p> <h3 id="data-preprocessing">Data Preprocessing</h3> <p>We constructed a new data preprocessing pipeline compatible with the Amazon Reviews 2018 dataset, which provides more valid visual and textual data for multimodal adaptation across multiple product categories.</p> <h2 id="preliminary-results">Preliminary Results</h2> <p>We have successfully replicated baseline models and conducted extensive experiments. The performance aligns well with reported results in the original papers.</p> <h3 id="baseline-performance">Baseline Performance</h3> <p><strong>ActionPiece vs TIGER on Amazon Reviews:</strong></p> <table> <thead> <tr> <th>Dataset</th> <th>Model</th> <th>R@5</th> <th>N@5</th> <th>R@10</th> <th>N@10</th> </tr> </thead> <tbody> <tr> <td>Sports &amp; Outdoors</td> <td>ActionPiece</td> <td>0.0316</td> <td>0.0205</td> <td>0.0500</td> <td>0.0264</td> </tr> <tr> <td>Sports &amp; Outdoors</td> <td>TIGER</td> <td>0.0341</td> <td>0.0221</td> <td>0.0518</td> <td>0.0279</td> </tr> <tr> <td>Beauty</td> <td>ActionPiece</td> <td>0.0511</td> <td>0.0340</td> <td>0.0775</td> <td>0.0424</td> </tr> <tr> <td>Beauty</td> <td>TIGER</td> <td>0.0627</td> <td>0.0418</td> <td>0.0914</td> <td>0.0511</td> </tr> </tbody> </table> <h3 id="clairo-performance">CLAIRO Performance</h3> <p>Due to the long preprocessing time for visual information, we have currently conducted experiments on the CDs and Vinyls dataset:</p> <table> <thead> <tr> <th>Model</th> <th>R@5</th> <th>N@5</th> <th>R@10</th> <th>N@10</th> </tr> </thead> <tbody> <tr> <td>ActionPiece</td> <td>0.0544</td> <td>0.0359</td> <td>0.0830</td> <td>0.0451</td> </tr> <tr> <td><strong>CLAIRO</strong></td> <td><strong>0.0561</strong></td> <td><strong>0.0450</strong></td> <td><strong>0.0713</strong></td> <td><strong>0.0499</strong></td> </tr> </tbody> </table> <p><strong>Key Findings:</strong></p> <ul> <li>CLAIRO achieves consistent improvements over the text-only baseline across most metrics</li> <li> <strong>25.6% improvement in NDCG@5</strong> (from 0.0359 to 0.0451) and <strong>10.6% improvement in NDCG@10</strong> (from 0.0451 to 0.0499) on the CDs and Vinyl dataset</li> <li>Visual features provide complementary signals for item representation, particularly beneficial for products where visual cues play a significant role in user decisions</li> <li> <strong>Computational efficiency maintained</strong>: 50% reduction in processing overhead through PCA compression while preserving 95% of embedding information</li> <li>Performance across different Amazon Reviews categories varies significantly, so results are only comparable within the same category</li> </ul> <h3 id="important-insights">Important Insights</h3> <p>From our preliminary experiments, we identified several challenges:</p> <ul> <li> <strong>Representation collapse</strong>: Direct concatenation of embeddings from different modalities without proper regularization can cause the model to over-rely on one modality</li> <li> <strong>Shared vocabulary limitations</strong>: The current shared token vocabulary struggles with separately encoding modality-specific patterns, motivating our investigation into discrete tokenization with separate codebooks</li> </ul> <h2 id="contributions">Contributions</h2> <p>Our work makes several contributions to the multimodal generative recommendation field:</p> <ol> <li> <p><strong>First multimodal extension of ActionPiece</strong>: We present the first multimodal extension of ActionPiece’s collaborative tokenization framework, demonstrating its adaptability beyond text-only scenarios</p> </li> <li> <p><strong>Overcoming VQ-VAE limitations with RQ-VAE</strong>: We systematically analyzed and addressed the codebook collapse issue in VQ-VAE by implementing residual quantization, enabling hierarchical representation learning that captures the multi-level structure of human decision-making processes</p> </li> <li> <p><strong>BPE-inspired multimodal token reconstruction</strong>: We introduce a novel byte pair encoding-inspired algorithm that identifies and merges co-occurring patterns across text and vision modalities, effectively aligning latent codes with human-interpretable behavioral units while improving sequence coherence and codebook utilization</p> </li> <li> <p><strong>Efficient high-dimensional multimodal fusion</strong>: We developed a learned attention mechanism combined with PCA compression that preserves 95% of embedding information while reducing computational overhead by 50%, enabling practical deployment of multimodal recommendation systems</p> </li> <li> <p><strong>Empirical evidence</strong>: Our results demonstrate that proper multimodal integration can achieve <strong>25.6% improvement in recommendation accuracy</strong> while maintaining computational efficiency, providing strong evidence for the value of visual features in sequential recommendation tasks</p> </li> </ol> <h2 id="future-work">Future Work</h2> <h3 id="1-advanced-multimodal-tokenization">1. Advanced Multimodal Tokenization</h3> <p>We are investigating different methods to efficiently merge textual and visual features while avoiding representation collapse. Drawing inspiration from MQL4GRec, we plan to implement <strong>discrete multimodal tokenization using separate codebooks</strong> for each modality:</p> <ul> <li>Train independent quantizers (e.g., RQ-VAE) for textual and visual embeddings</li> <li>Use distinct token prefixes such as <code class="language-plaintext highlighter-rouge">&lt;A_i&gt;</code> for textual tokens and <code class="language-plaintext highlighter-rouge">&lt;a_j&gt;</code> for visual tokens</li> <li>Explore various quantization strategies including single-level vs multi-level codebooks, Optimized Product Quantization (OPQ), and Vector-Quantized Variational Encoder (VQ-VAE)</li> </ul> <h3 id="2-higher-performance-encoders">2. Higher-Performance Encoders</h3> <p>While our current implementation leverages CLIP ViT-L/14 as the visual encoder and SentenceT5, we plan to investigate more powerful architectures to enhance CLAIRO’s representation quality and improve model performance.</p> <h3 id="3-cross-attention-mechanisms">3. Cross-Attention Mechanisms</h3> <p>To enable deeper multimodal interaction beyond simple concatenation or early fusion, we plan to investigate cross-attention architectures at different stages:</p> <ul> <li>Implement fusion with cross-attention before the encoder (computationally efficient)</li> <li>Explore decoder-stage late fusion if time allows</li> <li>Investigate gating mechanisms to dynamically balance the contribution of different modalities</li> </ul> <h3 id="4-comprehensive-evaluation-and-ablation-studies">4. Comprehensive Evaluation and Ablation Studies</h3> <p>We plan to conduct extensive ablation studies using modality masking strategies:</p> <ul> <li>Train variants with text-only, vision-only, and both modalities to quantify individual and joint contributions</li> <li>Perform component ablation to assess the importance of specific architectural choices</li> <li>Analyze attention patterns through visualization to understand cross-modal interactions</li> <li>Validate our approach on the large-scale <strong>Amazon Reviews 2023 dataset</strong> (2M+ items, 33 categories)</li> <li>Test hypothesis that visually-distinctive categories (e.g., Clothing, Home Decor) benefit more from visual features than text-heavy categories (e.g., Books)</li> </ul> <h2 id="skills-developed">Skills Developed</h2> <ul> <li>Advanced PyTorch implementation</li> <li>Multimodal representation learning</li> <li>Tokenization algorithms and collaborative filtering</li> <li>Vision Transformers and CLIP models</li> <li>Variational Autoencoder architectures (RQ-VAE, VQ-VAE)</li> <li>Large-scale dataset processing and experimentation</li> <li>Research paper reproduction and baseline evaluation</li> </ul> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Zhaodong Liu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>